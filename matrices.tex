\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}

\title{Merkintöjä}
\author{heikkila.elias }
\date{November 2021}

\setlength{\parindent}{0em}
\begin{document}
	Define $2 \times 2$ Pauli matrices: $\mathbf{I}_2, \mathbf{X} = \begin{pmatrix} 0 && 1 \\ 1 && 0 \end{pmatrix}, \mathbf{Z} = \begin{pmatrix} 1 && 0 \\ 0 && -1 \end{pmatrix}, \mathbf{Y} = i\mathbf{X}\mathbf{Z} = \begin{pmatrix} 0 && -i \\ i && 0 \end{pmatrix}$. We need the coefficient $i$ in front of $\mathbf{XZ}$ to make the matrix Hermitian.
	Let $m \in \mathbb{N}$ and $N = 2^m$. Define a $N \times N$ Pauli matrix with binary vectors $\mathbf{a} = (a_1, ..., a_m), \mathbf{b} = (b_1,..., b_m) \in \mathbb{F}_2^m$ as follows:
	\begin{align*}
		\mathbf{E}(\mathbf{a}, \mathbf{b}) = i^{\mathbf{a}\mathbf{b}^T} \mathbf{X}^{a_1}\mathbf{Z}^{b_1} \otimes \cdots \otimes \mathbf{X}^{a_m}\mathbf{Z}^{b_m}
	\end{align*}
	$\mathbf{E}(\mathbf{a}, \mathbf{b})$ is clearly Hermitian as a Kronecker product of Hermitian matrices.
	Let's recall that Walsh-Hadamard matrix can be written in a form (where this product form is proved?)
	% TODO find out a reference for the product form
	\begin{align*}
		\mathbf{H} = \prod_{j=1}^m \frac{1}{\sqrt{2}} (\mathbf{I} + i\mathbf{E}(\mathbf{e}_j, \mathbf{e}_j))
	\end{align*}
	
	Now by simply opening the product, we get
	\begin{align*}
		\prod_{j=1}^m \frac{1}{\sqrt{2}} (\mathbf{I} + i\mathbf{E}(\mathbf{e}_j, \mathbf{e}_j)) = \frac{1}{\sqrt{N}} \sum_{\mathbf{a} \in \mathbb{F}^m_2} i^{w(\mathbf{a})}\mathbf{E}(\mathbf{a},\mathbf{a}).
	\end{align*}
	
	Walsh-Hadamard matrix belongs to the Clifford group as it permutes the Pauli group. Walsh-Hadamard matrix corresponds to the symplectic matrix $\mathbf{F} = \begin{bmatrix}
		\mathbf{0} & \mathbf{I} \\
		\mathbf{I} & \mathbf{0}
	\end{bmatrix}$
	so this means that if we conjugate $\mathbf{E}(\mathbf{a},\mathbf{b})$ with the Walsh-Hadamard matrix $\mathbf{H}$, we will swap the places of $\mathbf{a}$ and $\mathbf{b}$: $\mathbf{H}^H\mathbf{E}(\mathbf{a},\mathbf{b})\mathbf{H} = \pm \mathbf{E}(\left[\mathbf{a},\mathbf{b}\right]\mathbf{F}) = \pm \mathbf{E}(\mathbf{b},\mathbf{a})$. Now it is clear that Walsh-Hadamard matrix gives an isomorphism between the $\mathcal{X}$-group: $\mathcal{X} = \{\mathbf{E}(\mathbf{a},\mathbf{0}) \mid \mathbf{a} \in \mathbb{F}^m_2 \}$ and $\mathcal{Z}$-group: $\mathcal{Z} = \{\mathbf{E}(\mathbf{0},\mathbf{b}) \mid \mathbf{b} \in \mathbb{F}^m_2 \}$ since $\mathbf{H}^H\mathcal{X}\mathbf{H} = \mathcal{Z}$. The $\mathcal{X}$-group is a maximal commuting subgroup of the Pauli group.
	
	Now the symplectic matrix $\mathbf{T}_\mathbf{S} = \begin{bmatrix} \mathbf{I} & \mathbf{S} \\ \mathbf{0} & \mathbf{I} \end{bmatrix}$, where $\mathbf{S}$ is a binary symmetric matrix, corresponds to the Clifford operator $\mathbf{g} = \text{diag}(i^{\mathbf{v}\mathbf{S}\mathbf{v}^T})$. (This is where the generalization happens when we consider arbitrary binary matrices or matrices with half-elements in the generalized setting).
	
	%The columns of the Walsh-Hadamard matrix are common eigenvectors (with eigenvalue $\pm 1$) of the $\mathcal{X}$-group. First notice that the columns of Walsh-Hadamard matrix can be written as Kronecker products of vectors $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ and $\begin{pmatrix} 1 \\ -1 \end{pmatrix}$.
	
	% TODO proof
	%(needs proof) e.g. $\begin{pmatrix} 1 \\ 1 \end{pmatrix} \otimes \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} 1 \\ -1 \\ 1 \\ -1 \end{pmatrix}$. Notice also that  Now $\mathbf{X}\begin{pmatrix} 1 \\ -1 \end{pmatrix} = - \begin{pmatrix} 1 \\ -1 \end{pmatrix}$. Now for example
	%\begin{align*}
	%	 \begin{pmatrix} 1 \\ -1 \\ 1 \\ -1 \end{pmatrix} \mathbf{I} \otimes \mathbf{X} = \begin{pmatrix} 1 \\ 1 \end{pmatrix} \mathbf{I} \otimes \begin{pmatrix} 1 \\ -1 \end{pmatrix} \mathbf{X} = - \begin{pmatrix} 1 \\ -1 \\ 1 \\ -1 \end{pmatrix}
	%\end{align*}
	
	When we conjugate Walsh-Hadamard matrix with $\mathbf{g}$, we get a matrix of codewords determined by $\mathbf{S}$, where the codewords are the columns (or rows) of the following matrix:
	% TODO is here after conjugation with g some sign..?
	\begin{align*}
		&\mathbf{W} = \mathbf{g}^H\mathbf{H}\mathbf{g} = \frac{1}{\sqrt{N}}\mathbf{g}^H\left(\sum_{\mathbf{a} \in \mathbb{F}^m_2} i^{w(\mathbf{a})}\mathbf{E}(\mathbf{a},\mathbf{a})\right)\mathbf{g} = \frac{1}{\sqrt{N}}\sum_{\mathbf{a} \in \mathbb{F}^m_2} i^{w(\mathbf{a})}\mathbf{g}^H\mathbf{E}(\mathbf{a},\mathbf{a})\mathbf{g} = \\ &\frac{1}{\sqrt{N}}\sum_{\mathbf{a} \in \mathbb{F}^m_2} i^{w(\mathbf{a})}\mathbf{E}(\left[\mathbf{a},\mathbf{a}\right]\mathbf{T}_\mathbf{S}) = \frac{1}{\sqrt{N}}\sum_{\mathbf{a} \in \mathbb{F}^m_2} i^{w(\mathbf{a})}\mathbf{E}(\mathbf{a},\mathbf{a}\mathbf{S} + \mathbf{a})
	\end{align*}
	
	Now we can also write this in a product form
	\begin{align*}
		\frac{1}{\sqrt{N}} \sum_{\mathbf{a} \in \mathbb{F}^m_2} i^{w(\mathbf{a})}\mathbf{E}(\mathbf{a},\mathbf{a}\mathbf{S} + \mathbf{a}) = \prod_{j = 1}^m\frac{1}{\sqrt{2}}(\mathbf{I} + i\mathbf{E}(\mathbf{e}_j, \mathbf{e}_j(\mathbf{S} + \mathbf{I})))
	\end{align*}
	%TODO IS HERE SOME SIGN IN THE PRODUCT FORM??
	Now calculate a "shift" of the codeword matrix:
	\begin{align*}
		\mathbf{E}(\mathbf{e}_j, \mathbf{0})\mathbf{W} = &\frac{1}{\sqrt{N}} \sum_{\mathbf{a} \in \mathbb{F}^m_2} i^{w(\mathbf{a})}\mathbf{E}(\mathbf{e}_j,\mathbf{0})\mathbf{E}(\mathbf{a},\mathbf{a}\mathbf{S} + \mathbf{a}) = \frac{1}{\sqrt{N}} \sum_{\mathbf{a} \in \mathbb{F}^m_2} i^{w(\mathbf{a})}i^{-\mathbf{e}_j(\mathbf{a}\mathbf{S} + \mathbf{a})}\mathbf{E}(\mathbf{a} + \mathbf{e}_j,\mathbf{a}\mathbf{S} + \mathbf{a})  = \\ = &\frac{1}{\sqrt{N}} \sum_{\mathbf{a} \in \mathbb{F}^m_2} i^{w(\mathbf{a})-\mathbf{a}(\mathbf{S} + \mathbf{I})\mathbf{e}_j^T}\mathbf{E}(\mathbf{a} + \mathbf{e}_j,\mathbf{a}\mathbf{S} + \mathbf{a})
	\end{align*}
	
	\vspace{10mm}
	
	Denote by $\overline{\mathbf{W}}$ the elementwise complex conjugate of the matrix $\mathbf{W}$. Consider the pointwise product
	% TODO find out what the pointwise product is
	\begin{align*}
		\overline{\mathbf{W}} \odot \mathbf{E}(\mathbf{e}_n, 0)\mathbf{W} = \frac{1}{N} \sum_a i^{-w(a)}(-1)^{a(aS + a)^T} E(a, a(S + I)) \sum_b i^{w(b) - b(S + I)e_n^T} E(b + e_n, b(S + I))
	\end{align*}
	If $a \neq b \implies E(a, c) \odot E(b, d) = 0$ so this means that
	
	CORRECT THE SIGNS BELOW!!!
	
	\begin{align*}
		&\frac{1}{N} \sum_a i^{-w(a)} E(a, a(S + I)) \sum_b i^{w(b) - b(S + I)e_n^T} E(b + e_n, b(S + I)) =\\ &\frac{1}{N}\sum_{a,b} \delta_{a, b + e_n} i^{-w(a)} i^{w(b)-b(S+I)e_n^T} E(a, a(S+I)) \odot E(b + e_n, b(S+I)) =\\
		&\frac{1}{N}\sum_{b} i^{w(b)-w(b+e_n)-b(S+I)e_n^T}E(b + e_n, e_n(S+I)) = \frac{1}{N} \sum_{b} i^{bb^T - (b+e_n)(b+e_n)^T - bS_n - be_n^T}E(b+e_n, S_n + e_n) = \\ &\frac{1}{N} \sum_{b} i^{b(e_n + S_n)^T - 1}E(b+e_n,S_n+e_n)
	\end{align*}
	The second vector $S_n+e_n$, that determines the signs, stays the same over the whole sum. Now notice that $E(b+e_n, S_n + e_n) = i^{(b+e_n)(S_n+e_n)^T}E(b + e_n, 0)E(0,S_n + e_n)$.
	\begin{align*}
		&\frac{1}{N} \sum_{b} i^{b(e_n + S_n)^T - 1}E(b+e_n,S_n+e_n)
		= \frac{1}{N} \sum_{b} i^{b(e_n + S_n)^T-1}i^{(b+e_n)(S_n+e_n)^T}E(b + e_n, 0)E(0,S_n + e_n) = \\ &\frac{1}{N} \left(\sum_{b} (-1)^{b(e_n+S_n)^T} E(b+e_n, 0) \right) i^{e_nS_n^T}E(0, S_n + e_n)
	\end{align*}
	Now 
	$\left[(-1)^{b(e_n+S_n)^T}\right]_{b \in \mathbb{F}_2^m}$ is a row of WHT matrix. 
	The $E(b+e_n,0)$-part fills the matrix in such a way that every column is a polynomial permutation of the row $\left[(-1)^{b(e_n+S_n)^T}\right]_{b \in \mathbb{F}_2^m}$ (this needs rigorous proof, but it holds expirimentally in Mathematica and intuitionally). Since the WHT-rows are $\pm 1$ eigenvectors with respect to polynomial permutations, we will get a matrix with $\pm 1$ the same WHT-row as every column. \newpage 
	
	If this matrix is multiplied by the WHT matrix $H$ we will get a following kind of matrix:
	
	\begin{align*}
		H \frac{1}{N} \left(\sum_{b} (-1)^{b(e_n+S_n)^T} E(b+e_n, 0) \right) i^{e_nS_n^T}E(0, S_n + e_n) = \frac{i^{e_nS_n^T}}{N}ME(0, S_n + e_n)
	\end{align*}
	where $M = \begin{pmatrix} 0 & 0 & \cdots & 0 \\
		\vdots & \vdots &  & \vdots \\
		0 & 0 &  & 0 \\
		m_1 & m_2 & \cdots & m_N \\
		0 & 0 & & 0 \\
		\vdots & \vdots &  & \vdots \\
		0 & 0 & \cdots & 0 \\
	\end{pmatrix}$ and now since $E(0, S_n + e_n)$ is just a diagonal matrix with some sign we get just the expected result. The whole matrix $\frac{i^{e_nS_n^T}}{N}ME(0, S_n + e_n)$ is just of form:
	\begin{align*}
		\frac{i^{e_nS_n^T}}{N} \begin{pmatrix} 0 & 0 & \cdots & 0 \\
			\vdots & \vdots &  & \vdots \\
			0 & 0 &  & 0 \\
			s_1m_1 & s_2m_2 & \cdots & s_Nm_N \\
			0 & 0 & & 0 \\
			\vdots & \vdots &  & \vdots \\
			0 & 0 & \cdots & 0 \\
		\end{pmatrix}
	\end{align*}
	
	This tells us that the shift-and-multiply operation gives an unique peak at a coordinate of a codeword, which determines a row of the $S$-matrix. The sign bits $s_1,...,s_N$ determine a bit at the $b$-vector, when the chirp is written in form $\left[i^{vSv^T + 2vb^T}\right]_{v \in \mathbb{F}^m_2}$.
	
	%\xrightarrow[\text{WHT}]{\text{}} \sum_b i^{w(b)-w(a)-b(S+I)e_n^T}E(e_n(S+I), b + e_n)
	%
	%\begin{align*}
	%    &E(a,a(S+I)) \cdot E(b+e_n,b(S+I)) = \delta_{a,b+e_n} E(a,  (b+e_n)(S+I) + b(S+I)) =  \delta_{a,b+e_n} E(a,  e_n(S+I))  =\\ &\delta_{a,b+e_n} E(a,  S_n+e_n)
	%\end{align*}
\end{document}